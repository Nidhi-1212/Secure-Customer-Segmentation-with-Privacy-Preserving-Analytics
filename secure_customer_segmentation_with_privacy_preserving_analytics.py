# -*- coding: utf-8 -*-
"""Secure Customer Segmentation with Privacy-Preserving Analytics

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V2FHBDVeCNO3V8zT8Jt7bwX3jU_mwEoK
"""

!pip install numpy pandas scikit-learn pycryptodome

from google.colab import files
uploaded = files.upload()

import pandas as pd
data = pd.read_csv('ecommerce_data.csv')  # Use the dataset "commerce_data.csv"

print(data.columns)

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Load dataset
data = pd.read_csv('ecommerce_data.csv')  # Replace with the actual path to your dataset

# Display the first few rows to verify data loading
print(data.head())

# Preprocess data
# We will use 'Time on App', 'Time on Website', 'Length of Membership', 'Yearly Amount Spent' for clustering
features = data[['Time on App', 'Time on Website', 'Length of Membership', 'Yearly Amount Spent']]

# Standardize the features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Split data into training and test sets
X_train, X_test = train_test_split(scaled_features, test_size=0.2, random_state=42)

# Display the first few rows of the scaled training data
print(pd.DataFrame(X_train, columns=['Time on App', 'Time on Website', 'Length of Membership', 'Yearly Amount Spent']).head())

# Generates RSA keys and uses them to encrypt and decrypt data.
# The `encrypt_data` function encrypts each item in the data list,
# while `decrypt_data` decrypts the encrypted items and decodes them.

from Crypto.PublicKey import RSA
from Crypto.Cipher import PKCS1_OAEP
import numpy as np

# Generate RSA keys
key = RSA.generate(2048)
public_key = key.publickey()
encryptor = PKCS1_OAEP.new(public_key)
decryptor = PKCS1_OAEP.new(key)

# Encrypt data
def encrypt_data(data):
    encrypted_data = []
    for item in data:
        encrypted_data.append(encryptor.encrypt(item.encode('utf-8')))
    return encrypted_data

# Decrypt data
def decrypt_data(encrypted_data):
    decrypted_data = []
    for item in encrypted_data:
        decrypted_data.append(decryptor.decrypt(item).decode('utf-8'))
    return decrypted_data

# Example encryption and decryption
encrypted_features = encrypt_data(np.array_str(X_train))
decrypted_features = decrypt_data(encrypted_features)

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans

# Clean column names by stripping any leading/trailing whitespace or tabs
data.columns = data.columns.str.strip()

# Select relevant features for clustering
# Assuming we want to cluster based on time spent, membership length, and yearly spending
features = data[['Time on App', 'Time on Website', 'Length of Membership', 'Yearly Amount Spent']]

# Ensure decrypted features are numerical
# Using string split instead of np.fromstring for safer handling
cleaned_decrypted_features = []
valid_indices = []  # Keep track of valid indices corresponding to valid features
max_length = None  # Track the maximum length for padding/truncating

for idx, item in enumerate(decrypted_features):
    try:
        # Safely split the string into a list of numbers
        numeric_array = [float(x) for x in item.split(',') if x.strip()]

        # Set max_length for padding/truncating
        if max_length is None:
            max_length = len(numeric_array)

        # Ensure the array has the expected length by padding or truncating
        if len(numeric_array) < max_length:
            numeric_array = np.pad(numeric_array, (0, max_length - len(numeric_array)), mode='constant')
        elif len(numeric_array) > max_length:
            numeric_array = numeric_array[:max_length]

        cleaned_decrypted_features.append(numeric_array)
        valid_indices.append(idx)  # Keep track of the valid index
    except ValueError:
        # If the string cannot be converted, handle the exception
        print(f"Skipping invalid entry at index {idx}: {item}")

# Convert the cleaned list to a numpy array if it's not empty
if cleaned_decrypted_features:
    cleaned_decrypted_features = np.array(cleaned_decrypted_features)
else:
    raise ValueError("No valid features found in the decrypted data.")

# Check if we have valid features before proceeding with KMeans
if cleaned_decrypted_features.shape[1] == 0:
    raise ValueError("No valid features found after cleaning the decrypted features.")

# K-Means clustering
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(cleaned_decrypted_features)
clusters = kmeans.predict(cleaned_decrypted_features)

# Create a new column for cluster information in the original data
# Set clusters only for the rows with valid decrypted data
data['Cluster'] = np.nan  # Initialize with NaN
data.loc[valid_indices, 'Cluster'] = clusters  # Assign clusters only to valid indices

# Display the first few rows of the data to verify the clusters
print(data.head())

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split

# Load dataset
data = pd.read_csv('ecommerce_data.csv')  # Replace with the actual path to your dataset

# Preprocess data
features = data[['Time on App', 'Time on Website', 'Length of Membership', 'Yearly Amount Spent']]

# Standardize the features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Perform K-means clustering
kmeans = KMeans(n_clusters=5, random_state=42)  # You can adjust the number of clusters
data['Cluster'] = kmeans.fit_predict(scaled_features)

# Analyze cluster profiles
cluster_summary = data.groupby('Cluster').agg({
    'Time on App': 'mean',
    'Time on Website': 'mean',
    'Length of Membership': 'mean',
    'Yearly Amount Spent': 'mean'
})

print(cluster_summary)